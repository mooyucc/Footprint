//
//  DeepSeekProvider.swift
//  Footprint
//
//  Created on 2025/01/27.
//  DeepSeek API æœåŠ¡å®ç°
//

import Foundation
import SwiftData
import Vision
import UIKit

/// DeepSeek API æœåŠ¡å®ç°
/// ä½¿ç”¨ DeepSeek APIï¼ˆå…¼å®¹ OpenAI æ ¼å¼ï¼‰æä¾› AI åŠŸèƒ½
class DeepSeekProvider: AIServiceProtocol {
    static let shared = DeepSeekProvider()
    
    private let apiKey: String
    private let baseURL = "https://api.deepseek.com/v1"
    private let requestTimeout: TimeInterval = 30.0
    
    // DeepSeek æ¨¡å‹é…ç½®
    private let chatModel = "deepseek-chat"  // æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
    // æ³¨æ„ï¼šDeepSeek API å½“å‰å¯èƒ½ä¸æ”¯æŒå¤šæ¨¡æ€ï¼ˆå›¾ç‰‡ï¼‰ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å‹
    // private let visionModel = "deepseek-v2"  // å¤šæ¨¡æ€æ¨¡å‹ï¼ˆæš‚ä¸æ”¯æŒï¼‰
    
    private init() {
        // ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è¯»å–API Key
        // ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > Info.plist
        if let key = ProcessInfo.processInfo.environment["DeepSeekAPIKey"],
           !key.isEmpty {
            self.apiKey = key
            print("âœ… [DeepSeek] ä»ç¯å¢ƒå˜é‡è¯»å–API Key")
        } else if let key = Bundle.main.object(forInfoDictionaryKey: "DeepSeekAPIKey") as? String,
                  !key.isEmpty {
            self.apiKey = key
            print("âœ… [DeepSeek] ä»Info.plistè¯»å–API Key")
        } else {
            fatalError("âŒ [DeepSeek] API Keyæœªé…ç½®ï¼Œè¯·åœ¨Info.plistä¸­æ·»åŠ DeepSeekAPIKeyæˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        }
        
        print("ğŸ¤– [DeepSeek] æœåŠ¡å·²åˆå§‹åŒ–ï¼ŒAPI Key: \(String(apiKey.prefix(8)))...")
    }
    
    // MARK: - AIServiceProtocol Implementation
    
    func generateNotes(
        from images: [Data],
        location: String,
        country: String,
        date: Date
    ) async throws -> String {
        print("ğŸ¤– [DeepSeek] å¼€å§‹ç”Ÿæˆç¬”è®°ï¼Œåœ°ç‚¹: \(location), å›½å®¶: \(country)")
        
        // æ„å»º Prompt
        let dateFormatter = DateFormatter()
        dateFormatter.dateFormat = "yyyyå¹´MMæœˆddæ—¥"
        let dateString = dateFormatter.string(from: date)
        
        var promptText = "ä½ æ˜¯ä¸€ä½æ—…è¡Œä½œå®¶ã€‚æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€æ®µæ—…è¡Œç¬”è®°ï¼š\n- åœ°ç‚¹ï¼š\(location)\n- å›½å®¶ï¼š\(country)\n- è®¿é—®æ—¥æœŸï¼š\(dateString)"
        
        // å¦‚æœæœ‰ç…§ç‰‡ï¼Œä½¿ç”¨ Apple Vision API è¯†åˆ«å›¾ç‰‡å†…å®¹
        if !images.isEmpty {
            print("ğŸ“¸ [Vision] å¼€å§‹ä½¿ç”¨Apple Vision APIè¯†åˆ«\(images.count)å¼ ç…§ç‰‡...")
            
            var imageDescriptions: [String] = []
            for (index, imageData) in images.prefix(3).enumerated() {
                if let description = await analyzeImageWithVision(imageData) {
                    imageDescriptions.append("ç…§ç‰‡\(index + 1)ï¼š\(description)")
                    print("âœ… [Vision] ç…§ç‰‡\(index + 1)è¯†åˆ«æˆåŠŸï¼š\(description.prefix(50))...")
                } else {
                    print("âš ï¸ [Vision] ç…§ç‰‡\(index + 1)è¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡")
                }
            }
            
            if !imageDescriptions.isEmpty {
                promptText += "\n- ç…§ç‰‡å†…å®¹æè¿°ï¼š\n\(imageDescriptions.joined(separator: "\n"))"
                print("âœ… [Vision] å›¾ç‰‡è¯†åˆ«å®Œæˆï¼Œå…±è¯†åˆ«\(imageDescriptions.count)å¼ ç…§ç‰‡")
            } else {
                promptText += "\n- ç”¨æˆ·ä¸Šä¼ äº†\(images.count)å¼ ç…§ç‰‡ï¼ˆå›¾ç‰‡è¯†åˆ«æœªæˆåŠŸï¼‰"
                print("âš ï¸ [Vision] æ‰€æœ‰ç…§ç‰‡è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æè¿°")
            }
            
            promptText += "\n\nè¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ç…§ç‰‡å†…å®¹æè¿°ï¼Œç”Ÿæˆä¸€æ®µæ—…è¡Œç¬”è®°ï¼Œ**ä¸¥æ ¼é™åˆ¶åœ¨144å­—ä»¥å†…**ã€‚è¦æ±‚ï¼š\n1. ç»“åˆç…§ç‰‡ä¸­å®é™…çœ‹åˆ°çš„åœºæ™¯å’Œå†…å®¹\n2. ç»“åˆè¿™ä¸ªåœ°ç‚¹çš„ç‰¹è‰²å’Œæ–‡åŒ–èƒŒæ™¯\n3. ä½“ç°å½“åœ°æ–‡åŒ–æˆ–è‡ªç„¶é£è²Œ\n4. è¯­è¨€è‡ªç„¶æµç•…ï¼Œå¸¦æœ‰ä¸ªäººæ„Ÿå—\n5. ä½¿ç”¨ä¸­æ–‡è¾“å‡º\n6. **é‡è¦ï¼šå­—æ•°å¿…é¡»ä¸¥æ ¼æ§åˆ¶åœ¨144å­—ä»¥å†…ï¼Œä¸è¦è¶…è¿‡**"
        } else {
            promptText += "\n\nè¯·ç”Ÿæˆä¸€æ®µæ—…è¡Œç¬”è®°ï¼Œ**ä¸¥æ ¼é™åˆ¶åœ¨144å­—ä»¥å†…**ã€‚è¦æ±‚ï¼š\n1. æè¿°è¿™ä¸ªåœ°ç‚¹çš„ç‰¹è‰²\n2. ä½“ç°å½“åœ°æ–‡åŒ–æˆ–è‡ªç„¶é£è²Œ\n3. è¯­è¨€è‡ªç„¶æµç•…ï¼Œå¸¦æœ‰ä¸ªäººæ„Ÿå—\n4. ä½¿ç”¨ä¸­æ–‡è¾“å‡º\n5. **é‡è¦ï¼šå­—æ•°å¿…é¡»ä¸¥æ ¼æ§åˆ¶åœ¨144å­—ä»¥å†…ï¼Œä¸è¦è¶…è¿‡**"
        }
        
        let messages: [ChatMessage] = [
            .user([.text(promptText)])
        ]
        
        let response = try await callChatAPI(messages: messages, model: chatModel)
        
        // é™åˆ¶å†…å®¹é•¿åº¦åœ¨144å­—ä»¥å†…
        return limitTo144Characters(response)
    }
    
    func generateTripDescription(
        for destinations: [TravelDestination]
    ) async throws -> String {
        print("ğŸ¤– [DeepSeek] å¼€å§‹ç”Ÿæˆæ—…ç¨‹æè¿°ï¼Œç›®çš„åœ°æ•°é‡: \(destinations.count)")
        
        guard !destinations.isEmpty else {
            throw AIError.invalidInput("ç›®çš„åœ°åˆ—è¡¨ä¸ºç©º")
        }
        
        // æ„å»ºç›®çš„åœ°ä¿¡æ¯ï¼ˆåŒ…å«ç¬”è®°ï¼‰
        let dateFormatter = DateFormatter()
        dateFormatter.dateFormat = "yyyy-MM-dd"
        
        var destinationsInfo = destinations.map { dest in
            let dateStr = dateFormatter.string(from: dest.visitDate)
            var info = "- \(dest.name) (\(dest.country)) - \(dateStr)"
            // å¦‚æœæœ‰ç¬”è®°ï¼Œæ·»åŠ åˆ°ä¿¡æ¯ä¸­
            if !dest.notes.isEmpty {
                info += "\n  ç¬”è®°ï¼š\(dest.notes)"
            }
            return info
        }.joined(separator: "\n\n")
        
        // æ”¶é›†æ‰€æœ‰ç›®çš„åœ°çš„ç…§ç‰‡
        var allImages: [(destination: String, images: [Data])] = []
        for destination in destinations {
            if !destination.photoDatas.isEmpty {
                allImages.append((destination: destination.name, images: destination.photoDatas))
            }
        }
        
        var promptText = "ä½ æ˜¯ä¸€ä½æ—…è¡Œä½œå®¶ã€‚åˆ†æä»¥ä¸‹æ—…ç¨‹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µæ—…ç¨‹æ•´ä½“æè¿°ï¼š\n\nç›®çš„åœ°åˆ—è¡¨ï¼š\n\(destinationsInfo)"
        
        // å¦‚æœæœ‰ç…§ç‰‡ï¼Œä½¿ç”¨ Apple Vision API è¯†åˆ«å›¾ç‰‡å†…å®¹
        if !allImages.isEmpty {
            print("ğŸ“¸ [Vision] å¼€å§‹ä½¿ç”¨Apple Vision APIè¯†åˆ«æ—…ç¨‹ä¸­çš„ç…§ç‰‡ï¼Œå…±\(allImages.count)ä¸ªç›®çš„åœ°æœ‰ç…§ç‰‡...")
            
            var imageDescriptions: [String] = []
            var totalProcessed = 0
            let maxImagesPerDestination = 2 // æ¯ä¸ªç›®çš„åœ°æœ€å¤šè¯†åˆ«2å¼ ç…§ç‰‡ï¼Œé¿å…å¤ªå¤š
            
            for (destName, images) in allImages {
                var destImageDescriptions: [String] = []
                
                for (index, imageData) in images.prefix(maxImagesPerDestination).enumerated() {
                    if let description = await analyzeImageWithVision(imageData) {
                        destImageDescriptions.append("  - ç…§ç‰‡\(index + 1)ï¼š\(description)")
                        totalProcessed += 1
                        print("âœ… [Vision] \(destName) ç…§ç‰‡\(index + 1)è¯†åˆ«æˆåŠŸ")
                    }
                }
                
                if !destImageDescriptions.isEmpty {
                    imageDescriptions.append("\(destName)çš„ç…§ç‰‡ï¼š\n\(destImageDescriptions.joined(separator: "\n"))")
                }
            }
            
            if !imageDescriptions.isEmpty {
                promptText += "\n\næ—…ç¨‹ä¸­çš„ç…§ç‰‡å†…å®¹æè¿°ï¼š\n\(imageDescriptions.joined(separator: "\n\n"))"
                print("âœ… [Vision] å›¾ç‰‡è¯†åˆ«å®Œæˆï¼Œå…±è¯†åˆ«\(totalProcessed)å¼ ç…§ç‰‡ï¼Œæ¥è‡ª\(imageDescriptions.count)ä¸ªç›®çš„åœ°")
            } else {
                promptText += "\n\næ—…ç¨‹ä¸­åŒ…å«ç…§ç‰‡ï¼Œä½†å›¾ç‰‡è¯†åˆ«æœªæˆåŠŸ"
                print("âš ï¸ [Vision] æ‰€æœ‰ç…§ç‰‡è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æè¿°")
            }
            
            promptText += "\n\nè¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ç…§ç‰‡å†…å®¹æè¿°å’Œå„ä¸ªåœ°ç‚¹çš„ç¬”è®°ï¼Œç”Ÿæˆä¸€æ®µæ—…ç¨‹æ•´ä½“æè¿°ï¼Œ**ä¸¥æ ¼é™åˆ¶åœ¨300å­—ä»¥å†…**ã€‚è¦æ±‚ï¼š\n1. ç»“åˆç…§ç‰‡ä¸­å®é™…çœ‹åˆ°çš„åœºæ™¯å’Œå†…å®¹\n2. å‚è€ƒå„ä¸ªåœ°ç‚¹çš„ç¬”è®°å†…å®¹ï¼Œä½“ç°æ—…ç¨‹çš„è¿è´¯æ€§å’Œç‰¹è‰²\n3. è¯­è¨€è‡ªç„¶æµç•…ï¼Œå¸¦æœ‰ä¸ªäººæ„Ÿå—\n4. ä½¿ç”¨ä¸­æ–‡è¾“å‡º\n5. **é‡è¦ï¼šå­—æ•°å¿…é¡»ä¸¥æ ¼æ§åˆ¶åœ¨300å­—ä»¥å†…ï¼Œä¸è¦è¶…è¿‡**"
        } else {
            promptText += "\n\nè¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å„ä¸ªåœ°ç‚¹çš„ç¬”è®°ï¼Œç”Ÿæˆä¸€æ®µæ—…ç¨‹æ•´ä½“æè¿°ï¼Œ**ä¸¥æ ¼é™åˆ¶åœ¨300å­—ä»¥å†…**ã€‚è¦æ±‚ï¼š\n1. å‚è€ƒå„ä¸ªåœ°ç‚¹çš„ç¬”è®°å†…å®¹ï¼Œä½“ç°æ—…ç¨‹çš„è¿è´¯æ€§å’Œç‰¹è‰²\n2. è¯­è¨€è‡ªç„¶æµç•…ï¼Œå¸¦æœ‰ä¸ªäººæ„Ÿå—\n3. ä½¿ç”¨ä¸­æ–‡è¾“å‡º\n4. **é‡è¦ï¼šå­—æ•°å¿…é¡»ä¸¥æ ¼æ§åˆ¶åœ¨300å­—ä»¥å†…ï¼Œä¸è¦è¶…è¿‡**"
        }
        
        let messages: [ChatMessage] = [
            .user([.text(promptText)])
        ]
        
        let response = try await callChatAPI(messages: messages, model: chatModel)
        
        // é™åˆ¶å†…å®¹é•¿åº¦åœ¨300å­—ä»¥å†…
        return limitTo300Characters(response)
    }
    
    func analyzeImages(_ images: [Data]) async throws -> ImageAnalysisResult {
        print("ğŸ¤– [DeepSeek] å¼€å§‹åˆ†æç…§ç‰‡ï¼Œç…§ç‰‡æ•°é‡: \(images.count)")
        
        guard !images.isEmpty else {
            throw AIError.invalidInput("ç…§ç‰‡æ•°ç»„ä¸ºç©º")
        }
        
        // ä½¿ç”¨ Apple Vision API è¯†åˆ«å›¾ç‰‡å†…å®¹
        print("ğŸ“¸ [Vision] ä½¿ç”¨Apple Vision APIè¯†åˆ«ç…§ç‰‡...")
        
        var allDescriptions: [String] = []
        var allObservations: [String] = []
        
        for (index, imageData) in images.prefix(3).enumerated() {
            if let (description, observations) = await analyzeImageWithVisionForAnalysis(imageData) {
                allDescriptions.append("ç…§ç‰‡\(index + 1)ï¼š\(description)")
                allObservations.append(contentsOf: observations)
                print("âœ… [Vision] ç…§ç‰‡\(index + 1)è¯†åˆ«æˆåŠŸ")
            }
        }
        
        // å°†è¯†åˆ«ç»“æœå‘é€ç»™ DeepSeek è¿›è¡Œç»“æ„åŒ–åˆ†æ
        var promptText = "ä½ æ˜¯ä¸€ä¸ªç…§ç‰‡åˆ†æåŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹å›¾ç‰‡è¯†åˆ«ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–çš„åˆ†æï¼š\n\n"
        
        if !allDescriptions.isEmpty {
            promptText += "å›¾ç‰‡æè¿°ï¼š\n\(allDescriptions.joined(separator: "\n"))\n\n"
        }
        
        if !allObservations.isEmpty {
            promptText += "è¯†åˆ«åˆ°çš„ç‰©ä½“å’Œåœºæ™¯ï¼š\(allObservations.joined(separator: "ã€"))\n\n"
        }
        
        promptText += "è¯·ç”Ÿæˆåˆ†æç»“æœï¼ŒåŒ…æ‹¬ï¼š\n1. åœºæ™¯ç±»å‹ï¼ˆè‡ªç„¶ã€åŸå¸‚ã€å»ºç­‘ã€ç¾é£Ÿã€æ–‡åŒ–ç­‰ï¼‰\n2. ä¸»è¦ç‰©ä½“æˆ–åœ°æ ‡ï¼ˆä»è¯†åˆ«ç»“æœä¸­æå–ï¼‰\n3. ç…§ç‰‡çš„æ•´ä½“æè¿°ï¼ˆåŸºäºè¯†åˆ«ç»“æœï¼‰\n4. å»ºè®®çš„æ ‡ç­¾ï¼ˆ3-5ä¸ªï¼‰\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼ï¼š{\"sceneType\": \"åœºæ™¯ç±»å‹\", \"mainSubjects\": [\"ç‰©ä½“1\", \"ç‰©ä½“2\"], \"description\": \"æè¿°\", \"suggestedTags\": [\"æ ‡ç­¾1\", \"æ ‡ç­¾2\"]}\nä½¿ç”¨ä¸­æ–‡è¾“å‡ºã€‚"
        
        let messages: [ChatMessage] = [.user([.text(promptText)])]
        
        let response = try await callChatAPI(messages: messages, model: chatModel)
        
        // è§£æ JSON å“åº”
        return try parseImageAnalysisResult(from: response)
    }
    
    func generateTags(
        for destination: TravelDestination
    ) async throws -> [String] {
        print("ğŸ¤– [DeepSeek] å¼€å§‹ç”Ÿæˆæ ‡ç­¾ï¼Œåœ°ç‚¹: \(destination.name)")
        
        let messages: [ChatMessage] = [
            .user([.text("æ ¹æ®ä»¥ä¸‹ç›®çš„åœ°ä¿¡æ¯ï¼Œç”Ÿæˆ3-5ä¸ªæ ‡ç­¾ï¼š\n- åœ°ç‚¹ï¼š\(destination.name)\n- å›½å®¶ï¼š\(destination.country)\n- ç¬”è®°ï¼š\(destination.notes ?? "æ— ")\n\nè¯·ç”Ÿæˆæ ‡ç­¾ï¼Œæ¯ä¸ªæ ‡ç­¾2-4ä¸ªå­—ï¼Œä½¿ç”¨ä¸­æ–‡ï¼Œä»¥é€—å·åˆ†éš”ã€‚ä¾‹å¦‚ï¼šæ–‡åŒ–, å†å², å»ºç­‘, æ¨è")])
        ]
        
        let response = try await callChatAPI(messages: messages, model: chatModel)
        
        // è§£ææ ‡ç­¾ï¼ˆé€—å·åˆ†éš”ï¼‰
        let tags = response
            .components(separatedBy: ",")
            .map { $0.trimmingCharacters(in: .whitespacesAndNewlines) }
            .filter { !$0.isEmpty }
        
        return Array(tags.prefix(5)) // æœ€å¤š5ä¸ªæ ‡ç­¾
    }
    
    // MARK: - Private Methods
    
    /// è°ƒç”¨ DeepSeek Chat API
    private func callChatAPI(messages: [ChatMessage], model: String) async throws -> String {
        let url = URL(string: "\(baseURL)/chat/completions")!
        
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.timeoutInterval = requestTimeout
        
        // é™åˆ¶æœ€å¤§tokenæ•°
        // å¯¹äºç¬”è®°ç”Ÿæˆï¼šä¸­æ–‡å¹³å‡æ¯ä¸ªå­—çº¦1-2ä¸ªtokenï¼Œ144å­—çº¦éœ€è¦200-300 tokens
        // å¯¹äºæ—…ç¨‹æè¿°ï¼š300å­—çº¦éœ€è¦400-600 tokens
        // æ ¹æ®æ¶ˆæ¯å†…å®¹åˆ¤æ–­æ˜¯ç¬”è®°è¿˜æ˜¯æ—…ç¨‹æè¿°
        let promptText = messages.compactMap { msg -> String? in
            if case .user(let items) = msg, let first = items.first, case .text(let text) = first {
                return text
            }
            return nil
        }.joined()
        
        let maxTokens: Int
        if promptText.contains("300å­—") {
            maxTokens = 600  // æ—…ç¨‹æè¿°ï¼š300å­—
        } else if promptText.contains("144å­—") {
            maxTokens = 300  // ç¬”è®°ï¼š144å­—
        } else {
            maxTokens = 1000  // é»˜è®¤å€¼
        }
        
        let requestBody = ChatCompletionRequest(
            model: model,
            messages: messages,
            temperature: 0.7,
            maxTokens: maxTokens
        )
        
        request.httpBody = try JSONEncoder().encode(requestBody)
        
        print("ğŸ¤– [DeepSeek] å‘é€è¯·æ±‚ï¼Œæ¨¡å‹: \(model), æ¶ˆæ¯æ•°: \(messages.count)")
        
        let (data, response) = try await URLSession.shared.data(for: request)
        
        guard let httpResponse = response as? HTTPURLResponse else {
            throw AIError.networkError("æ— æ•ˆçš„HTTPå“åº”")
        }
        
        guard httpResponse.statusCode == 200 else {
            let errorMessage = String(data: data, encoding: .utf8) ?? "æœªçŸ¥é”™è¯¯"
            print("âŒ [DeepSeek] APIé”™è¯¯ï¼ŒçŠ¶æ€ç : \(httpResponse.statusCode), é”™è¯¯: \(errorMessage)")
            throw AIError.apiError("APIé”™è¯¯: \(httpResponse.statusCode)")
        }
        
        let chatResponse = try JSONDecoder().decode(ChatCompletionResponse.self, from: data)
        
        guard let content = chatResponse.choices.first?.message.content else {
            throw AIError.invalidResponse("å“åº”ä¸­æ²¡æœ‰å†…å®¹")
        }
        
        print("âœ… [DeepSeek] è¯·æ±‚æˆåŠŸï¼Œè¿”å›å†…å®¹é•¿åº¦: \(content.count)")
        
        // æ³¨æ„ï¼šå­—æ•°é™åˆ¶åœ¨å„è‡ªçš„ç”Ÿæˆæ–¹æ³•ä¸­å¤„ç†ï¼Œè¿™é‡Œè¿”å›åŸå§‹å†…å®¹
        return content
    }
    
    // MARK: - Vision API Methods
    
    /// ä½¿ç”¨ Apple Vision API è¯†åˆ«å›¾ç‰‡å†…å®¹ï¼ˆç”¨äºç¬”è®°ç”Ÿæˆï¼‰
    /// - Parameter imageData: å›¾ç‰‡æ•°æ®
    /// - Returns: å›¾ç‰‡çš„æ–‡å­—æè¿°
    private func analyzeImageWithVision(_ imageData: Data) async -> String? {
        guard let uiImage = UIImage(data: imageData) else {
            print("âŒ [Vision] æ— æ³•ä»Dataåˆ›å»ºUIImage")
            return nil
        }
        
        guard let cgImage = uiImage.cgImage else {
            print("âŒ [Vision] æ— æ³•è·å–CGImage")
            return nil
        }
        
        return await withCheckedContinuation { (continuation: CheckedContinuation<String?, Never>) in
            var descriptions: [String] = []
            var hasResumed = false
            
            // ç”¨äºç¡®ä¿åª resume ä¸€æ¬¡
            let resumeOnce: (String?) -> Void = { result in
                guard !hasResumed else { return }
                hasResumed = true
                continuation.resume(returning: result)
            }
            
            // ä½¿ç”¨ VNRecognizeTextRequest è¯†åˆ«æ–‡å­—
            let textRequest = VNRecognizeTextRequest { request, error in
                if let error = error {
                    print("âš ï¸ [Vision] æ–‡å­—è¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
                }
                
                if let observations = request.results as? [VNRecognizedTextObservation] {
                    let recognizedStrings = observations.compactMap { observation in
                        observation.topCandidates(1).first?.string
                    }
                    
                    if !recognizedStrings.isEmpty {
                        descriptions.append("æ–‡å­—ï¼š\(recognizedStrings.joined(separator: " "))")
                    }
                }
            }
            textRequest.recognitionLanguages = ["zh-Hans", "en"] // æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡
            textRequest.recognitionLevel = .accurate
            
            // ä½¿ç”¨ VNClassifyImageRequest åˆ†ç±»å›¾ç‰‡åœºæ™¯
            let classifyRequest = VNClassifyImageRequest { request, error in
                if let error = error {
                    print("âš ï¸ [Vision] å›¾ç‰‡åˆ†ç±»é”™è¯¯: \(error.localizedDescription)")
                }
                
                if let observations = request.results as? [VNClassificationObservation] {
                    // è·å–ç½®ä¿¡åº¦æœ€é«˜çš„3ä¸ªåˆ†ç±»
                    let topClassifications = observations.prefix(3).compactMap { observation -> String? in
                        guard observation.confidence > 0.3 else { return nil }
                        return observation.identifier
                    }
                    
                    if !topClassifications.isEmpty {
                        descriptions.append("åœºæ™¯ï¼š\(topClassifications.joined(separator: "ã€"))")
                    }
                }
                
                // åœ¨åœºæ™¯åˆ†ç±»å®Œæˆå resumeï¼ˆè¿™æ˜¯æœ€åä¸€ä¸ªè¯·æ±‚çš„å›è°ƒï¼‰
                let result = descriptions.isEmpty ? nil : descriptions.joined(separator: "ï¼›")
                resumeOnce(result)
            }
            
            // æ‰§è¡Œæ‰€æœ‰è¯·æ±‚ï¼ˆæ–‡å­—è¯†åˆ« + åœºæ™¯åˆ†ç±»ï¼‰
            let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
            do {
                try handler.perform([textRequest, classifyRequest])
            } catch {
                print("âŒ [Vision] æ‰§è¡ŒVisionè¯·æ±‚å¤±è´¥: \(error.localizedDescription)")
                resumeOnce(nil)
            }
        }
    }
    
    /// ä½¿ç”¨ Apple Vision API è¯†åˆ«å›¾ç‰‡å†…å®¹ï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰
    /// - Parameter imageData: å›¾ç‰‡æ•°æ®
    /// - Returns: (æè¿°, è¯†åˆ«çš„ç‰©ä½“åˆ—è¡¨)
    private func analyzeImageWithVisionForAnalysis(_ imageData: Data) async -> (description: String, observations: [String])? {
        guard let uiImage = UIImage(data: imageData),
              let cgImage = uiImage.cgImage else {
            return nil
        }
        
        return await withCheckedContinuation { continuation in
            var descriptions: [String] = []
            var observations: [String] = []
            
            let textRequest = VNRecognizeTextRequest { request, error in
                guard let observations = request.results as? [VNRecognizedTextObservation] else { return }
                let strings = observations.compactMap { $0.topCandidates(1).first?.string }
                if !strings.isEmpty {
                    descriptions.append("æ–‡å­—ï¼š\(strings.joined(separator: " "))")
                }
            }
            textRequest.recognitionLanguages = ["zh-Hans", "en"]
            textRequest.recognitionLevel = .accurate
            
            let classifyRequest = VNClassifyImageRequest { request, error in
                guard let classObservations = request.results as? [VNClassificationObservation] else {
                    continuation.resume(returning: descriptions.isEmpty ? nil : (descriptions.joined(separator: "ï¼›"), observations))
                    return
                }
                
                let topClasses = classObservations.prefix(5).compactMap { observation -> String? in
                    guard observation.confidence > 0.3 else { return nil }
                    observations.append(observation.identifier)
                    return observation.identifier
                }
                
                if !topClasses.isEmpty {
                    descriptions.append("åœºæ™¯åˆ†ç±»ï¼š\(topClasses.joined(separator: "ã€"))")
                }
                
                continuation.resume(returning: descriptions.isEmpty ? nil : (descriptions.joined(separator: "ï¼›"), observations))
            }
            
            let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
            do {
                try handler.perform([textRequest, classifyRequest])
            } catch {
                print("âŒ [Vision] æ‰§è¡ŒVisionè¯·æ±‚å¤±è´¥: \(error.localizedDescription)")
                continuation.resume(returning: nil)
            }
        }
    }
    
    /// é™åˆ¶æ–‡æœ¬é•¿åº¦åœ¨æŒ‡å®šå­—ç¬¦æ•°ä»¥å†…
    private func limitTextToLength(_ text: String, maxLength: Int) -> String {
        // ä¸­æ–‡å­—ç¬¦æŒ‰1ä¸ªå­—ç¬¦è®¡ç®—
        if text.count <= maxLength {
            return text
        }
        
        // æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦ï¼Œå°½é‡åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·å¤„æˆªæ–­
        let truncated = String(text.prefix(maxLength))
        
        // å°è¯•åœ¨æœ€åä¸€ä¸ªæ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
        let punctuationMarks: [Character] = ["ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", ".", "!", "?", ","]
        var bestIndex = truncated.count
        
        for mark in punctuationMarks {
            if let range = truncated.range(of: String(mark), options: .backwards, range: truncated.startIndex..<truncated.endIndex) {
                let index = truncated.distance(from: truncated.startIndex, to: range.upperBound)
                if index <= maxLength && index > max(maxLength - 50, 0) { // åœ¨æœ€å50ä¸ªå­—ç¬¦å†…å¯»æ‰¾
                    bestIndex = index
                    break
                }
            }
        }
        
        return String(truncated.prefix(bestIndex)).trimmingCharacters(in: .whitespacesAndNewlines)
    }
    
    /// é™åˆ¶æ–‡æœ¬é•¿åº¦åœ¨144ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰ä»¥å†…
    private func limitTo144Characters(_ text: String) -> String {
        return limitTextToLength(text, maxLength: 144)
    }
    
    /// é™åˆ¶æ–‡æœ¬é•¿åº¦åœ¨300ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰ä»¥å†…
    private func limitTo300Characters(_ text: String) -> String {
        return limitTextToLength(text, maxLength: 300)
    }
    
    /// è§£æç…§ç‰‡åˆ†æç»“æœ
    private func parseImageAnalysisResult(from jsonString: String) throws -> ImageAnalysisResult {
        // å°è¯•æå– JSONï¼ˆå¯èƒ½è¢« ```json åŒ…è£¹ï¼‰
        var jsonStr = jsonString.trimmingCharacters(in: .whitespacesAndNewlines)
        if jsonStr.hasPrefix("```json") {
            jsonStr = String(jsonStr.dropFirst(7))
        }
        if jsonStr.hasPrefix("```") {
            jsonStr = String(jsonStr.dropFirst(3))
        }
        if jsonStr.hasSuffix("```") {
            jsonStr = String(jsonStr.dropLast(3))
        }
        jsonStr = jsonStr.trimmingCharacters(in: .whitespacesAndNewlines)
        
        guard let jsonData = jsonStr.data(using: .utf8),
              let dict = try? JSONSerialization.jsonObject(with: jsonData) as? [String: Any] else {
            // å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºç¡€ç»“æœ
            return ImageAnalysisResult(
                sceneType: nil,
                mainSubjects: [],
                description: jsonString,
                suggestedTags: []
            )
        }
        
        return ImageAnalysisResult(
            sceneType: dict["sceneType"] as? String,
            mainSubjects: dict["mainSubjects"] as? [String] ?? [],
            description: dict["description"] as? String ?? jsonString,
            suggestedTags: dict["suggestedTags"] as? [String] ?? []
        )
    }
}

// MARK: - Data Models

/// èŠå¤©æ¶ˆæ¯
/// å…¼å®¹ OpenAI/DeepSeek API æ ¼å¼
enum ChatMessage: Codable {
    case system(String)
    case user([ContentItem])
    case assistant(String)
    
    var role: String {
        switch self {
        case .system: return "system"
        case .user: return "user"
        case .assistant: return "assistant"
        }
    }
    
    enum CodingKeys: String, CodingKey {
        case role, content
    }
    
    func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        try container.encode(role, forKey: .role)
        
        switch self {
        case .system(let text), .assistant(let text):
            // ç³»ç»Ÿå’ŒåŠ©æ‰‹æ¶ˆæ¯ï¼šcontent æ˜¯å­—ç¬¦ä¸²
            try container.encode(text, forKey: .content)
        case .user(let items):
            // ç”¨æˆ·æ¶ˆæ¯ï¼šcontent å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆçº¯æ–‡æœ¬ï¼‰æˆ–æ•°ç»„ï¼ˆæ··åˆå†…å®¹ï¼‰
            if items.count == 1, case .text(let text) = items.first {
                // å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬é¡¹ï¼Œç›´æ¥ç¼–ç ä¸ºå­—ç¬¦ä¸²ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
                try container.encode(text, forKey: .content)
            } else {
                // å¤šä¸ªé¡¹æˆ–åŒ…å«å›¾ç‰‡ï¼šç¼–ç ä¸ºæ•°ç»„
                try container.encode(items, forKey: .content)
            }
        }
    }
    
    init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        let role = try container.decode(String.self, forKey: .role)
        
        switch role {
        case "system":
            let text = try container.decode(String.self, forKey: .content)
            self = .system(text)
        case "user":
            // å°è¯•è§£ç ä¸ºæ•°ç»„ï¼Œå¦‚æœå¤±è´¥åˆ™ä½œä¸ºå­—ç¬¦ä¸²
            if let items = try? container.decode([ContentItem].self, forKey: .content) {
                self = .user(items)
            } else if let text = try? container.decode(String.self, forKey: .content) {
                self = .user([.text(text)])
            } else {
                throw DecodingError.dataCorrupted(DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Unable to decode user content"))
            }
        case "assistant":
            let text = try container.decode(String.self, forKey: .content)
            self = .assistant(text)
        default:
            throw DecodingError.dataCorrupted(DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Unknown role: \(role)"))
        }
    }
}

/// å†…å®¹é¡¹ï¼ˆæ”¯æŒæ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
/// å…¼å®¹ OpenAI/DeepSeek API æ ¼å¼
enum ContentItem: Codable {
    case text(String)
    case imageURL(String)
    
    var text: String? {
        if case .text(let text) = self {
            return text
        }
        return nil
    }
    
    enum CodingKeys: String, CodingKey {
        case type, text
        case imageURL = "image_url"
    }
    
    func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        
        switch self {
        case .text(let text):
            try container.encode("text", forKey: .type)
            try container.encode(text, forKey: .text)
        case .imageURL(let url):
            try container.encode("image_url", forKey: .type)
            // OpenAI/DeepSeek æ ¼å¼ï¼šimage_url æ˜¯ä¸€ä¸ªåŒ…å« url çš„å¯¹è±¡
            let imageURLDict = ImageURLDict(url: url)
            try container.encode(imageURLDict, forKey: .imageURL)
        }
    }
    
    init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        let type = try container.decode(String.self, forKey: .type)
        
        switch type {
        case "text":
            let text = try container.decode(String.self, forKey: .text)
            self = .text(text)
        case "image_url":
            let imageURLDict = try container.decode(ImageURLDict.self, forKey: .imageURL)
            self = .imageURL(imageURLDict.url)
        default:
            throw DecodingError.dataCorrupted(DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Unknown type: \(type)"))
        }
    }
    
    /// å›¾ç‰‡URLå­—å…¸ç»“æ„ï¼ˆç¬¦åˆ OpenAI/DeepSeek æ ¼å¼ï¼‰
    private struct ImageURLDict: Codable {
        let url: String
    }
}

/// Chat Completion è¯·æ±‚
struct ChatCompletionRequest: Codable {
    let model: String
    let messages: [ChatMessage]
    let temperature: Double
    let maxTokens: Int
}

/// Chat Completion å“åº”
struct ChatCompletionResponse: Codable {
    let choices: [Choice]
    
    struct Choice: Codable {
        let message: Message
        
        struct Message: Codable {
            let content: String
        }
    }
}

/// AI é”™è¯¯ç±»å‹
enum AIError: Error, LocalizedError {
    case networkError(String)
    case apiError(String)
    case invalidResponse(String)
    case invalidInput(String)
    case encodingError(String)
    
    var errorDescription: String? {
        switch self {
        case .networkError(let msg): return "ç½‘ç»œé”™è¯¯: \(msg)"
        case .apiError(let msg): return "APIé”™è¯¯: \(msg)"
        case .invalidResponse(let msg): return "æ— æ•ˆå“åº”: \(msg)"
        case .invalidInput(let msg): return "æ— æ•ˆè¾“å…¥: \(msg)"
        case .encodingError(let msg): return "ç¼–ç é”™è¯¯: \(msg)"
        }
    }
}

